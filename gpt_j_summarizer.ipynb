{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNqZOgY1IRk0pV+JA/gqjBk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abaranguer/lab-aina/blob/main/gpt_j_summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-J Summarizer\n",
        "\n",
        "https://www.eleuther.ai/artifacts/gpt-j\n",
        "\n",
        "https://huggingface.co/docs/transformers/model_doc/gptj\n",
        "\n",
        "https://medium.com/@maliahrajan/revolutionise-your-q-a-bot-with-gpt-j-the-open-source-game-changer-as-a-replacement-for-gpt-3-216bc4362b53\n",
        "\n",
        "https://www.reddit.com/r/datascience/comments/10987al/can_gptj_be_used_for_text_summarization/"
      ],
      "metadata": {
        "id": "ohNaOQso0Qc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPTJForCausalLM, AutoTokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "JH78sn8q0Z5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def intro():\n",
        "  print('''\n",
        "  Hi!\n",
        "  I'm GPT-J, a large language model trained by EleutherAI.\n",
        "  Write, or copy, the text to summarize.\n",
        "\n",
        "  (Enter *END* to finish session)\n",
        "\n",
        "  ''')\n",
        "\n",
        "def mainLoop():\n",
        "  follow = True\n",
        "  prompt = ''\n",
        "\n",
        "  while follow:\n",
        "    print('FULL TEXT: ')\n",
        "    prompt = input()\n",
        "    if prompt == '*END*':\n",
        "      follow = False\n",
        "    else:\n",
        "      prompt = prompt.strip()\n",
        "\n",
        "      if prompt != '':\n",
        "        prompt = 'Summarize the following text:\\n' + prompt + '\\nsummary:'\n",
        "        input_ids = tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\").input_ids\n",
        "\n",
        "        gen_tokens = model.generate(\n",
        "            input_ids,\n",
        "            do_sample=True,\n",
        "            temperature=0.95,\n",
        "            max_length=400, # Increased max_length\n",
        "        )\n",
        "\n",
        "        textAndSummary = tokenizer.batch_decode(\n",
        "                          gen_tokens,\n",
        "                          skip_special_tokens=True)[0]\n",
        "\n",
        "        if '\\nsummary' in textAndSummary:\n",
        "            summary = textAndSummary.split('\\nsummary')[1].strip()\n",
        "        else:\n",
        "            # Default response\n",
        "            summary = \"Sorry, I couldn't generate a summary for that.\"\n",
        "\n",
        "\n",
        "        print('SUMMARY: ', summary)\n",
        "\n",
        "def goodbye():\n",
        "  print('Goodbye!')"
      ],
      "metadata": {
        "id": "rwDk9yMZ0gUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "  else:\n",
        "    device = 'cpu'\n",
        "\n",
        "  print('Using device:', device)\n",
        "  torch.device(device)\n",
        "\n",
        "  model = GPTJForCausalLM.from_pretrained(\n",
        "    \"EleutherAI/gpt-j-6B\",\n",
        "    revision=\"float16\",\n",
        "    dtype=torch.float16,\n",
        "  )\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
        "\n",
        "  intro()\n",
        "  mainLoop()\n",
        "  goodbye()"
      ],
      "metadata": {
        "id": "XPTCzHeR0liY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}