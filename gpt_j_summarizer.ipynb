{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOKgsd1XSwyqqbmW6imBaqF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abaranguer/lab-aina/blob/main/gpt_j_summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-J Summarizer\n",
        "\n",
        "https://www.eleuther.ai/artifacts/gpt-j\n",
        "\n",
        "https://huggingface.co/docs/transformers/model_doc/gptj\n",
        "\n",
        "https://medium.com/@maliahrajan/revolutionise-your-q-a-bot-with-gpt-j-the-open-source-game-changer-as-a-replacement-for-gpt-3-216bc4362b53\n",
        "\n",
        "https://www.reddit.com/r/datascience/comments/10987al/can_gptj_be_used_for_text_summarization/"
      ],
      "metadata": {
        "id": "ohNaOQso0Qc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPTJForCausalLM, AutoTokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "JH78sn8q0Z5y"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def intro():\n",
        "  print('''\n",
        "  Hi!\n",
        "  I'm GPT-J, a large language model trained by EleutherAI.\n",
        "  Write, or copy, the text to summarize.\n",
        "\n",
        "  (Enter *END* to finish session)\n",
        "\n",
        "  ''')\n",
        "\n",
        "def mainLoop():\n",
        "  follow = True\n",
        "  prompt = ''\n",
        "\n",
        "  while follow:\n",
        "    print('FULL TEXT: ')\n",
        "    prompt = input()\n",
        "    if prompt == '*END*':\n",
        "      follow = False\n",
        "    else:\n",
        "      prompt = prompt.strip()\n",
        "\n",
        "      if prompt != '':\n",
        "        prompt = 'Summarize the following text:\\n' + prompt + '\\nsummary:'\n",
        "        input_ids = tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\").input_ids\n",
        "\n",
        "        gen_tokens = model.generate(\n",
        "            input_ids,\n",
        "            do_sample=True,\n",
        "            temperature=0.95,\n",
        "            max_length=400, # Increased max_length\n",
        "        )\n",
        "\n",
        "        queryAndAnswer = tokenizer.batch_decode(\n",
        "                          gen_tokens,\n",
        "                          skip_special_tokens=True)[0]\n",
        "\n",
        "        if '\\nsummary' in queryAndAnswer:\n",
        "            summary = queryAndAnswer.split('\\nsummary')[1].strip()\n",
        "        else:\n",
        "            # Default response\n",
        "            summary = \"Sorry, I couldn't generate a summary for that.\"\n",
        "\n",
        "\n",
        "        print('SUMMARY: ', summary)\n",
        "\n",
        "def goodbye():\n",
        "  print('Goodbye!')"
      ],
      "metadata": {
        "id": "rwDk9yMZ0gUJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "  else:\n",
        "    device = 'cpu'\n",
        "\n",
        "  print('Using device:', device)\n",
        "  torch.device(device)\n",
        "\n",
        "  model = GPTJForCausalLM.from_pretrained(\n",
        "    \"EleutherAI/gpt-j-6B\",\n",
        "    revision=\"float16\",\n",
        "    dtype=torch.float16,\n",
        "  )\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
        "\n",
        "  intro()\n",
        "  mainLoop()\n",
        "  goodbye()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPTCzHeR0liY",
        "outputId": "15cd15c6-be4f-4243-bb53-b282d894ea0c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at EleutherAI/gpt-j-6B were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n",
            "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Hi!\n",
            "  I'm GPT-J, a large language model trained by EleutherAI.\n",
            "  Write, or copy, the text to summarize.\n",
            "\n",
            "  (Enter *END* to finish session)\n",
            "\n",
            "  \n",
            "FULL TEXT: \n",
            "  K-means clustering is an unsupervised learning algorithm used for data clustering, which groups unlabeled data points into groups or clusters.  It is one of the most popular clustering methods used in machine learning. Unlike supervised learning, the training data that this algorithm uses is unlabeled, meaning that data points do not have a defined classification structure.  While various types of clustering algorithms exist, including exclusive, overlapping, hierarchical and probabilistic, the k-means clustering algorithm is an example of an exclusive or “hard” clustering method. This form of grouping stipulates that a data point can exist in just one cluster. This type of cluster analysis is commonly used in data science for market segmentation, document clustering, image segmentation and image compression. The k-means algorithm is a widely used method in cluster analysis because it is efficient, effective and simple.  K-means is an iterative, centroid-based clustering algorithm that partitions a dataset into similar groups based on the distance between their centroids. The centroid, or cluster center, is either the mean or median of all the points within the cluster depending on the characteristics of the data.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SUMMARY:  : k-means clustering is an unsupervised learning algorithm used for data clustering, which groups unlabeled data points into groups or clusters. It is one of the most popular clustering methods used in machine learning. [Unsupervised learning is a discipline in the field of machine learning that focuses on the extraction of hidden patterns from data, and it has become one of the most important techniques in artificial intelligence. Unsupervised learning includes machine learning, pattern recognition and data mining as a subset (see supervised learning).] [unsupervised learning] [machine learning] [pattern recognition] [data mining] //This is an example of summarizing data.\n",
            "This is an\n",
            "FULL TEXT: \n",
            "  K-means clustering is an unsupervised learning algorithm used for data clustering, which groups unlabeled data points into groups or clusters.  It is one of the most popular clustering methods used in machine learning. Unlike supervised learning, the training data that this algorithm uses is unlabeled, meaning that data points do not have a defined classification structure.  While various types of clustering algorithms exist, including exclusive, overlapping, hierarchical and probabilistic, the k-means clustering algorithm is an example of an exclusive or “hard” clustering method. This form of grouping stipulates that a data point can exist in just one cluster. This type of cluster analysis is commonly used in data science for market segmentation, document clustering, image segmentation and image compression. The k-means algorithm is a widely used method in cluster analysis because it is efficient, effective and simple.  K-means is an iterative, centroid-based clustering algorithm that partitions a dataset into similar groups based on the distance between their centroids. The centroid, or cluster center, is either the mean or median of all the points within the cluster depending on the characteristics of the data.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SUMMARY:  :\n",
            "1) K-means clustering: A hard clustering algorithm that assigns unlabeled data objects to clusters or segments.\n",
            "2) k-means algorithm: An iterative, centroid-based clustering algorithm that partitions a dataset into similar groups based on the distance between their centroid\n",
            "3) Data clustering: Unsupervised learning used for group analysis of unlabeled data objects.\n",
            "4) Cluster analysis: A statistical clustering method used to group similar data objects into clusters or segments.\n",
            "5) Unsupervised learning: Machine learning that doesn’t require labeled data.\n",
            "K-Means Clustering - Clustering Data Points\n",
            "FULL TEXT: \n",
            "\"The flowers have been growing thorns for millions of years. For millions of years the sheep have been eating them just the same. And is it not a matter of consequence to try to understand why the flowers go to so much trouble to grow thorns which are never of any use to them? Is the warfare between the sheep and the flowers not important? Is this not of more consequence than a fat red-faced gentleman's sums? And if I know--I, myself--one flower which is unique in the world, which grows nowhere but on my planet, but which one little sheep can destroy in a single bite some morning, without even noticing what he is doing-- Oh! You think that is not important!\" His face turned from white to red as he continued: \"If some one loves a flower, of which just one single blossom grows in all the millions and millions of stars, it is enough to make him happy just to look at the stars. He can say to himself, 'Somewhere, my flower is there . . .' But if the sheep eats the flower, in one moment all his stars will be darkened . . . And you think that is not important!\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUMMARY:  : (a) The flowers have been growing thorns for millions of years - not an actual date - it's implied to be very long time periods. (b) There is a war between the flowers and sheep. A war that the flowers lose. (c) The flowers, through some process, have developed thorns that can't harm the sheep while sheep can eat the flowers (d) No matter how long the flowers have been in use, or how strong the thorns, they could always get more time and more energy to develop the thorns.\n",
            "\n",
            "2. What is the significance of the sheep? Who is the sheep? What does the sheep eat? Where is the description of the flower and its bloom? Which\n",
            "FULL TEXT: \n",
            "*END*\n",
            "Goodbye!\n"
          ]
        }
      ]
    }
  ]
}