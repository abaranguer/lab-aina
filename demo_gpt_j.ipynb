{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1cQA8rf3jRFwrfW0xGDgvAIiHzjANZAH3",
      "authorship_tag": "ABX9TyNTp4+++aEMFz8g+LUnEO1j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abaranguer/lab-aina/blob/main/demo_gpt_j.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-J chatbot\n",
        "\n",
        "### https://www.eleuther.ai/artifacts/gpt-j\n",
        "### https://huggingface.co/docs/transformers/model_doc/gptj\n",
        "### https://medium.com/@maliahrajan/revolutionise-your-q-a-bot-with-gpt-j-the-open-source-game-changer-as-a-replacement-for-gpt-3-216bc4362b53"
      ],
      "metadata": {
        "id": "JlI2TATozPiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPTJForCausalLM, AutoTokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "JiGVRZN8zZbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def intro():\n",
        "  print('''\n",
        "  Hi!\n",
        "  I'm GPT-J, a large language model trained by EleutherAI.\n",
        "  How can I help you today?\n",
        "\n",
        "  (Enter *END* to finish session)\n",
        "\n",
        "  ''')\n",
        "\n",
        "def mainLoop():\n",
        "  follow = True\n",
        "  prompt = ''\n",
        "\n",
        "  while follow:\n",
        "    print('You: ')\n",
        "    prompt = input()\n",
        "    if prompt == '*END*':\n",
        "      follow = False\n",
        "    else:\n",
        "      prompt = prompt.strip()\n",
        "\n",
        "      if prompt != '':\n",
        "        # Added Q: and A: to the prompt to encourage the model to follow the format\n",
        "        prompt = 'Q: ' + prompt + '\\nA:'\n",
        "        input_ids = tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\").input_ids\n",
        "\n",
        "        gen_tokens = model.generate(\n",
        "            input_ids,\n",
        "            do_sample=True,\n",
        "            temperature=0.9,\n",
        "            max_length=100,\n",
        "        )\n",
        "\n",
        "        queryAndAnswer = tokenizer.batch_decode(\n",
        "                          gen_tokens,\n",
        "                          skip_special_tokens=True)[0]\n",
        "\n",
        "        # Handle cases where the model doesn't output \"A:\"\n",
        "        if \"A:\" in queryAndAnswer:\n",
        "            answer = queryAndAnswer.split(\"A:\")[1].strip()\n",
        "            if \"Q:\" in answer:\n",
        "              answer = answer.split(\"Q:\")[0]\n",
        "        else:\n",
        "            # Default response\n",
        "            answer = \"Sorry, I couldn't generate a response for that.\"\n",
        "\n",
        "\n",
        "        print('Me: ', answer)\n",
        "\n",
        "def goodbye():\n",
        "  print('Goodbye!')"
      ],
      "metadata": {
        "id": "QcsOYn3s-Z4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "  else:\n",
        "    device = 'cpu'\n",
        "\n",
        "  print('Using device:', device)\n",
        "  torch.device(device)\n",
        "\n",
        "  model = GPTJForCausalLM.from_pretrained(\n",
        "    \"EleutherAI/gpt-j-6B\",\n",
        "    revision=\"float16\",\n",
        "    dtype=torch.float16,\n",
        "  )\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
        "\n",
        "  intro()\n",
        "  mainLoop()\n",
        "  goodbye()"
      ],
      "metadata": {
        "id": "s1aqCNT7gR-j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}